{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n023M1G3l4_r"
      },
      "source": [
        "# V-JEPA 2 Demo Notebook\n",
        "\n",
        "This tutorial provides an example of how to load the V-JEPA 2 model in vanilla PyTorch and HuggingFace, extract a video embedding, and then predict an action class. For more details about the paper and model weights, please see https://github.com/facebookresearch/vjepa2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwT2kQ2al4_s"
      },
      "source": [
        "First, let's import the necessary libraries and load the necessary functions for this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install all required packages\n",
        "!pip install decord transformers torch torchvision accelerate safetensors huggingface_hub"
      ],
      "metadata": {
        "id": "ZB3ytkeimHbj",
        "outputId": "807dda15-7fd9-4884-948b-a37f91f2947f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: decord in /usr/local/lib/python3.12/dist-packages (0.6.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (0.7.0)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from decord) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2026.1.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the V-JEPA2 repository\n",
        "!git clone https://github.com/facebookresearch/vjepa2.git\n",
        "%cd vjepa2\n",
        "\n",
        "# Install dependencies\n",
        "!pip install -e .\n",
        "!pip install decord transformers torch torchvision"
      ],
      "metadata": {
        "id": "n_k5bixEmULc",
        "outputId": "0e9c98e3-287e-4b20-ae6d-5cdc66e8bf48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'vjepa2'...\n",
            "remote: Enumerating objects: 310, done.\u001b[K\n",
            "remote: Counting objects:   1% (1/71)\u001b[K\rremote: Counting objects:   2% (2/71)\u001b[K\rremote: Counting objects:   4% (3/71)\u001b[K\rremote: Counting objects:   5% (4/71)\u001b[K\rremote: Counting objects:   7% (5/71)\u001b[K\rremote: Counting objects:   8% (6/71)\u001b[K\rremote: Counting objects:   9% (7/71)\u001b[K\rremote: Counting objects:  11% (8/71)\u001b[K\rremote: Counting objects:  12% (9/71)\u001b[K\rremote: Counting objects:  14% (10/71)\u001b[K\rremote: Counting objects:  15% (11/71)\u001b[K\rremote: Counting objects:  16% (12/71)\u001b[K\rremote: Counting objects:  18% (13/71)\u001b[K\rremote: Counting objects:  19% (14/71)\u001b[K\rremote: Counting objects:  21% (15/71)\u001b[K\rremote: Counting objects:  22% (16/71)\u001b[K\rremote: Counting objects:  23% (17/71)\u001b[K\rremote: Counting objects:  25% (18/71)\u001b[K\rremote: Counting objects:  26% (19/71)\u001b[K\rremote: Counting objects:  28% (20/71)\u001b[K\rremote: Counting objects:  29% (21/71)\u001b[K\rremote: Counting objects:  30% (22/71)\u001b[K\rremote: Counting objects:  32% (23/71)\u001b[K\rremote: Counting objects:  33% (24/71)\u001b[K\rremote: Counting objects:  35% (25/71)\u001b[K\rremote: Counting objects:  36% (26/71)\u001b[K\rremote: Counting objects:  38% (27/71)\u001b[K\rremote: Counting objects:  39% (28/71)\u001b[K\rremote: Counting objects:  40% (29/71)\u001b[K\rremote: Counting objects:  42% (30/71)\u001b[K\rremote: Counting objects:  43% (31/71)\u001b[K\rremote: Counting objects:  45% (32/71)\u001b[K\rremote: Counting objects:  46% (33/71)\u001b[K\rremote: Counting objects:  47% (34/71)\u001b[K\rremote: Counting objects:  49% (35/71)\u001b[K\rremote: Counting objects:  50% (36/71)\u001b[K\rremote: Counting objects:  52% (37/71)\u001b[K\rremote: Counting objects:  53% (38/71)\u001b[K\rremote: Counting objects:  54% (39/71)\u001b[K\rremote: Counting objects:  56% (40/71)\u001b[K\rremote: Counting objects:  57% (41/71)\u001b[K\rremote: Counting objects:  59% (42/71)\u001b[K\rremote: Counting objects:  60% (43/71)\u001b[K\rremote: Counting objects:  61% (44/71)\u001b[K\rremote: Counting objects:  63% (45/71)\u001b[K\rremote: Counting objects:  64% (46/71)\u001b[K\rremote: Counting objects:  66% (47/71)\u001b[K\rremote: Counting objects:  67% (48/71)\u001b[K\rremote: Counting objects:  69% (49/71)\u001b[K\rremote: Counting objects:  70% (50/71)\u001b[K\rremote: Counting objects:  71% (51/71)\u001b[K\rremote: Counting objects:  73% (52/71)\u001b[K\rremote: Counting objects:  74% (53/71)\u001b[K\rremote: Counting objects:  76% (54/71)\u001b[K\rremote: Counting objects:  77% (55/71)\u001b[K\rremote: Counting objects:  78% (56/71)\u001b[K\rremote: Counting objects:  80% (57/71)\u001b[K\rremote: Counting objects:  81% (58/71)\u001b[K\rremote: Counting objects:  83% (59/71)\u001b[K\rremote: Counting objects:  84% (60/71)\u001b[K\rremote: Counting objects:  85% (61/71)\u001b[K\rremote: Counting objects:  87% (62/71)\u001b[K\rremote: Counting objects:  88% (63/71)\u001b[K\rremote: Counting objects:  90% (64/71)\u001b[K\rremote: Counting objects:  91% (65/71)\u001b[K\rremote: Counting objects:  92% (66/71)\u001b[K\rremote: Counting objects:  94% (67/71)\u001b[K\rremote: Counting objects:  95% (68/71)\u001b[K\rremote: Counting objects:  97% (69/71)\u001b[K\rremote: Counting objects:  98% (70/71)\u001b[K\rremote: Counting objects: 100% (71/71)\u001b[K\rremote: Counting objects: 100% (71/71), done.\u001b[K\n",
            "remote: Compressing objects: 100% (38/38), done.\u001b[K\n",
            "remote: Total 310 (delta 45), reused 33 (delta 33), pack-reused 239 (from 3)\u001b[K\n",
            "Receiving objects: 100% (310/310), 576.64 KiB | 6.95 MiB/s, done.\n",
            "Resolving deltas: 100% (141/141), done.\n",
            "/content/vjepa2/vjepa2\n",
            "Obtaining file:///content/vjepa2/vjepa2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.12/dist-packages (from vjepa2==0.0.1) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from vjepa2==0.0.1) (0.24.0+cu126)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (from vjepa2==0.0.1) (2.19.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (from vjepa2==0.0.1) (0.24.0)\n",
            "Requirement already satisfied: iopath in /usr/local/lib/python3.12/dist-packages (from vjepa2==0.0.1) (0.1.10)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from vjepa2==0.0.1) (6.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from vjepa2==0.0.1) (2.0.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (from vjepa2==0.0.1) (4.12.0.88)\n",
            "Requirement already satisfied: submitit in /usr/local/lib/python3.12/dist-packages (from vjepa2==0.0.1) (1.5.4)\n",
            "Requirement already satisfied: braceexpand in /usr/local/lib/python3.12/dist-packages (from vjepa2==0.0.1) (0.1.7)\n",
            "Requirement already satisfied: webdataset in /usr/local/lib/python3.12/dist-packages (from vjepa2==0.0.1) (1.0.2)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (from vjepa2==0.0.1) (1.0.24)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from vjepa2==0.0.1) (4.57.6)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (from vjepa2==0.0.1) (0.18.1)\n",
            "Requirement already satisfied: decord in /usr/local/lib/python3.12/dist-packages (from vjepa2==0.0.1) (0.6.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from vjepa2==0.0.1) (2.2.2)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from vjepa2==0.0.1) (0.8.1)\n",
            "Requirement already satisfied: beartype in /usr/local/lib/python3.12/dist-packages (from vjepa2==0.0.1) (0.22.9)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from vjepa2==0.0.1) (5.9.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from vjepa2==0.0.1) (3.15.1)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.12/dist-packages (from vjepa2==0.0.1) (0.7.1)\n",
            "Requirement already satisfied: python-box in /usr/local/lib/python3.12/dist-packages (from vjepa2==0.0.1) (7.3.2)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (from vjepa2==0.0.1) (0.25.2)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.12/dist-packages (from vjepa2==0.0.1) (6.3.1)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.12/dist-packages (from vjepa2==0.0.1) (1.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2->vjepa2==0.0.1) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2->vjepa2==0.0.1) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2->vjepa2==0.0.1) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2->vjepa2==0.0.1) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2->vjepa2==0.0.1) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2->vjepa2==0.0.1) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2->vjepa2==0.0.1) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2->vjepa2==0.0.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2->vjepa2==0.0.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2->vjepa2==0.0.1) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2->vjepa2==0.0.1) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2->vjepa2==0.0.1) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2->vjepa2==0.0.1) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2->vjepa2==0.0.1) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2->vjepa2==0.0.1) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2->vjepa2==0.0.1) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2->vjepa2==0.0.1) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2->vjepa2==0.0.1) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2->vjepa2==0.0.1) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2->vjepa2==0.0.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2->vjepa2==0.0.1) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2->vjepa2==0.0.1) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2->vjepa2==0.0.1) (3.5.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from fire->vjepa2==0.0.1) (3.3.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->vjepa2==0.0.1) (0.2.14)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from iopath->vjepa2==0.0.1) (4.67.1)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.12/dist-packages (from iopath->vjepa2==0.0.1) (3.2.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.12/dist-packages (from jupyter->vjepa2==0.0.1) (6.5.7)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.12/dist-packages (from jupyter->vjepa2==0.0.1) (6.6.3)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.12/dist-packages (from jupyter->vjepa2==0.0.1) (7.16.6)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.12/dist-packages (from jupyter->vjepa2==0.0.1) (6.17.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.12/dist-packages (from jupyter->vjepa2==0.0.1) (7.7.1)\n",
            "Requirement already satisfied: jupyterlab in /usr/local/lib/python3.12/dist-packages (from jupyter->vjepa2==0.0.1) (4.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->vjepa2==0.0.1) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->vjepa2==0.0.1) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->vjepa2==0.0.1) (2025.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from peft->vjepa2==0.0.1) (25.0)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from peft->vjepa2==0.0.1) (1.12.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from peft->vjepa2==0.0.1) (0.7.0)\n",
            "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from peft->vjepa2==0.0.1) (0.36.0)\n",
            "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->vjepa2==0.0.1) (1.16.3)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.12/dist-packages (from scikit-image->vjepa2==0.0.1) (11.3.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image->vjepa2==0.0.1) (2.37.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image->vjepa2==0.0.1) (2026.1.14)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->vjepa2==0.0.1) (0.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.1 in /usr/local/lib/python3.12/dist-packages (from submitit->vjepa2==0.0.1) (3.1.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard->vjepa2==0.0.1) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard->vjepa2==0.0.1) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard->vjepa2==0.0.1) (3.10)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard->vjepa2==0.0.1) (5.29.5)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard->vjepa2==0.0.1) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->vjepa2==0.0.1) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard->vjepa2==0.0.1) (3.1.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->vjepa2==0.0.1) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers->vjepa2==0.0.1) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->vjepa2==0.0.1) (0.22.2)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb->vjepa2==0.0.1) (8.3.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb->vjepa2==0.0.1) (3.1.46)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb->vjepa2==0.0.1) (4.5.1)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb->vjepa2==0.0.1) (2.12.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb->vjepa2==0.0.1) (2.49.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->vjepa2==0.0.1) (4.0.12)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft->vjepa2==0.0.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb->vjepa2==0.0.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb->vjepa2==0.0.1) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb->vjepa2==0.0.1) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->vjepa2==0.0.1) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->vjepa2==0.0.1) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->vjepa2==0.0.1) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->vjepa2==0.0.1) (2026.1.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2->vjepa2==0.0.1) (1.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard->vjepa2==0.0.1) (3.0.3)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter->vjepa2==0.0.1) (1.8.15)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter->vjepa2==0.0.1) (7.34.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter->vjepa2==0.0.1) (7.4.9)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter->vjepa2==0.0.1) (0.2.1)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter->vjepa2==0.0.1) (1.6.0)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter->vjepa2==0.0.1) (26.2.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter->vjepa2==0.0.1) (6.5.1)\n",
            "Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter->vjepa2==0.0.1) (5.7.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets->jupyter->vjepa2==0.0.1) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets->jupyter->vjepa2==0.0.1) (3.6.10)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets->jupyter->vjepa2==0.0.1) (3.0.16)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.12/dist-packages (from jupyter-console->jupyter->vjepa2==0.0.1) (5.9.1)\n",
            "Requirement already satisfied: prompt-toolkit>=3.0.30 in /usr/local/lib/python3.12/dist-packages (from jupyter-console->jupyter->vjepa2==0.0.1) (3.0.52)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.12/dist-packages (from jupyter-console->jupyter->vjepa2==0.0.1) (2.19.2)\n",
            "Requirement already satisfied: async-lru>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab->jupyter->vjepa2==0.0.1) (2.1.0)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab->jupyter->vjepa2==0.0.1) (0.28.1)\n",
            "Requirement already satisfied: jupyter-lsp>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab->jupyter->vjepa2==0.0.1) (2.3.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab->jupyter->vjepa2==0.0.1) (2.14.0)\n",
            "Requirement already satisfied: jupyterlab-server<3,>=2.28.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab->jupyter->vjepa2==0.0.1) (2.28.0)\n",
            "Requirement already satisfied: notebook-shim>=0.2 in /usr/local/lib/python3.12/dist-packages (from jupyterlab->jupyter->vjepa2==0.0.1) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter->vjepa2==0.0.1) (4.13.5)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.12/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter->vjepa2==0.0.1) (6.3.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter->vjepa2==0.0.1) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter->vjepa2==0.0.1) (0.3.0)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter->vjepa2==0.0.1) (3.2.0)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter->vjepa2==0.0.1) (0.10.4)\n",
            "Requirement already satisfied: nbformat>=5.7 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter->vjepa2==0.0.1) (5.10.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter->vjepa2==0.0.1) (1.5.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.12/dist-packages (from notebook->jupyter->vjepa2==0.0.1) (25.1.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from notebook->jupyter->vjepa2==0.0.1) (2.1.0)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.12/dist-packages (from notebook->jupyter->vjepa2==0.0.1) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.12/dist-packages (from notebook->jupyter->vjepa2==0.0.1) (0.24.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.12/dist-packages (from notebook->jupyter->vjepa2==0.0.1) (1.3.3)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter->vjepa2==0.0.1) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter->vjepa2==0.0.1) (1.4.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->vjepa2==0.0.1) (5.0.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter->vjepa2==0.0.1) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter->vjepa2==0.0.1) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->jupyterlab->jupyter->vjepa2==0.0.1) (0.16.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->jupyter->vjepa2==0.0.1) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->jupyter->vjepa2==0.0.1) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->jupyter->vjepa2==0.0.1) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->jupyter->vjepa2==0.0.1) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->jupyter->vjepa2==0.0.1) (4.9.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->ipykernel->jupyter->vjepa2==0.0.1) (0.4)\n",
            "Requirement already satisfied: jupyter-events>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->vjepa2==0.0.1) (0.12.0)\n",
            "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->vjepa2==0.0.1) (0.5.4)\n",
            "Requirement already satisfied: overrides>=5.0 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->vjepa2==0.0.1) (7.7.0)\n",
            "Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->vjepa2==0.0.1) (1.9.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.12/dist-packages (from argon2-cffi->notebook->jupyter->vjepa2==0.0.1) (25.1.0)\n",
            "Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.12/dist-packages (from jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter->vjepa2==0.0.1) (2.17.0)\n",
            "Requirement already satisfied: json5>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter->vjepa2==0.0.1) (0.13.0)\n",
            "Requirement already satisfied: jsonschema>=4.18.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter->vjepa2==0.0.1) (4.26.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.12/dist-packages (from nbformat>=5.7->nbconvert->jupyter->vjepa2==0.0.1) (2.21.2)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.12/dist-packages (from terminado>=0.8.3->notebook->jupyter->vjepa2==0.0.1) (0.7.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->nbconvert->jupyter->vjepa2==0.0.1) (2.8.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter->vjepa2==0.0.1) (0.8.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter->vjepa2==0.0.1) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter->vjepa2==0.0.1) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter->vjepa2==0.0.1) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter->vjepa2==0.0.1) (0.30.0)\n",
            "Requirement already satisfied: python-json-logger>=2.0.4 in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->vjepa2==0.0.1) (4.0.0)\n",
            "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->vjepa2==0.0.1) (0.1.4)\n",
            "Requirement already satisfied: rfc3986-validator>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->vjepa2==0.0.1) (0.1.1)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter->vjepa2==0.0.1) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter->vjepa2==0.0.1) (2.23)\n",
            "Requirement already satisfied: fqdn in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->vjepa2==0.0.1) (1.5.1)\n",
            "Requirement already satisfied: isoduration in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->vjepa2==0.0.1) (20.11.0)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->vjepa2==0.0.1) (3.0.0)\n",
            "Requirement already satisfied: rfc3987-syntax>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->vjepa2==0.0.1) (1.1.0)\n",
            "Requirement already satisfied: uri-template in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->vjepa2==0.0.1) (1.3.0)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->vjepa2==0.0.1) (25.10.0)\n",
            "Requirement already satisfied: lark>=1.2.2 in /usr/local/lib/python3.12/dist-packages (from rfc3987-syntax>=1.1.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->vjepa2==0.0.1) (1.3.1)\n",
            "Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->vjepa2==0.0.1) (1.4.0)\n",
            "Building wheels for collected packages: vjepa2\n",
            "  Building editable for vjepa2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for vjepa2: filename=vjepa2-0.0.1-0.editable-py3-none-any.whl size=2308 sha256=16b8b193286febaf9cb12528ae4fcfd14cc8500cf36f98c59b2d5d67f4d51ecd\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-qfl8hgop/wheels/11/e3/a7/719966d3487b016995f4f22d87874efacfaa823b420185d973\n",
            "Successfully built vjepa2\n",
            "Installing collected packages: vjepa2\n",
            "  Attempting uninstall: vjepa2\n",
            "    Found existing installation: vjepa2 0.0.1\n",
            "    Uninstalling vjepa2-0.0.1:\n",
            "      Successfully uninstalled vjepa2-0.0.1\n",
            "Successfully installed vjepa2-0.0.1\n",
            "Requirement already satisfied: decord in /usr/local/lib/python3.12/dist-packages (0.6.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from decord) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2026.1.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "DB9Fm2Ngl4_s"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from decord import VideoReader\n",
        "from transformers import AutoVideoProcessor, AutoModel\n",
        "\n",
        "import src.datasets.utils.video.transforms as video_transforms\n",
        "import src.datasets.utils.video.volume_transforms as volume_transforms\n",
        "from src.models.attentive_pooler import AttentiveClassifier\n",
        "from src.models.vision_transformer import vit_giant_xformers_rope\n",
        "\n",
        "IMAGENET_DEFAULT_MEAN = (0.485, 0.456, 0.406)\n",
        "IMAGENET_DEFAULT_STD = (0.229, 0.224, 0.225)\n",
        "\n",
        "def load_pretrained_vjepa_pt_weights(model, pretrained_weights):\n",
        "    # Load weights of the VJEPA2 encoder\n",
        "    # The PyTorch state_dict is already preprocessed to have the right key names\n",
        "    pretrained_dict = torch.load(pretrained_weights, weights_only=True, map_location=\"cpu\")[\"encoder\"]\n",
        "    pretrained_dict = {k.replace(\"module.\", \"\"): v for k, v in pretrained_dict.items()}\n",
        "    pretrained_dict = {k.replace(\"backbone.\", \"\"): v for k, v in pretrained_dict.items()}\n",
        "    msg = model.load_state_dict(pretrained_dict, strict=False)\n",
        "    print(\"Pretrained weights found at {} and loaded with msg: {}\".format(pretrained_weights, msg))\n",
        "\n",
        "\n",
        "def load_pretrained_vjepa_classifier_weights(model, pretrained_weights):\n",
        "    # Load weights of the VJEPA2 classifier\n",
        "    # The PyTorch state_dict is already preprocessed to have the right key names\n",
        "    pretrained_dict = torch.load(pretrained_weights, weights_only=True, map_location=\"cpu\")[\"classifiers\"][0]\n",
        "    pretrained_dict = {k.replace(\"module.\", \"\"): v for k, v in pretrained_dict.items()}\n",
        "    msg = model.load_state_dict(pretrained_dict, strict=False)\n",
        "    print(\"Pretrained weights found at {} and loaded with msg: {}\".format(pretrained_weights, msg))\n",
        "\n",
        "\n",
        "def build_pt_video_transform(img_size):\n",
        "    short_side_size = int(256.0 / 224 * img_size)\n",
        "    # Eval transform has no random cropping nor flip\n",
        "    eval_transform = video_transforms.Compose(\n",
        "        [\n",
        "            video_transforms.Resize(short_side_size, interpolation=\"bilinear\"),\n",
        "            video_transforms.CenterCrop(size=(img_size, img_size)),\n",
        "            volume_transforms.ClipToTensor(),\n",
        "            video_transforms.Normalize(mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n",
        "        ]\n",
        "    )\n",
        "    return eval_transform\n",
        "\n",
        "\n",
        "def get_video():\n",
        "    vr = VideoReader(\"sample_video.mp4\")\n",
        "    # choosing some frames here, you can define more complex sampling strategy\n",
        "    frame_idx = np.arange(0, 128, 2)\n",
        "    video = vr.get_batch(frame_idx).asnumpy()\n",
        "    return video\n",
        "\n",
        "\n",
        "def forward_vjepa_video(model_hf, model_pt, hf_transform, pt_transform):\n",
        "    # Run a sample inference with VJEPA\n",
        "    with torch.inference_mode():\n",
        "        # Read and pre-process the image\n",
        "        video = get_video()  # T x H x W x C\n",
        "        video = torch.from_numpy(video).permute(0, 3, 1, 2)  # T x C x H x W\n",
        "        x_pt = pt_transform(video).cuda().unsqueeze(0)\n",
        "        x_hf = hf_transform(video, return_tensors=\"pt\")[\"pixel_values_videos\"].to(\"cuda\")\n",
        "        # Extract the patch-wise features from the last layer\n",
        "        out_patch_features_pt = model_pt(x_pt)\n",
        "        out_patch_features_hf = model_hf.get_vision_features(x_hf)\n",
        "\n",
        "    return out_patch_features_hf, out_patch_features_pt\n",
        "\n",
        "\n",
        "def get_vjepa_video_classification_results(classifier, out_patch_features_pt):\n",
        "    SOMETHING_SOMETHING_V2_CLASSES = json.load(open(\"ssv2_classes.json\", \"r\"))\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        out_classifier = classifier(out_patch_features_pt)\n",
        "\n",
        "    print(f\"Classifier output shape: {out_classifier.shape}\")\n",
        "\n",
        "    print(\"Top 5 predicted class names:\")\n",
        "    top5_indices = out_classifier.topk(5).indices[0]\n",
        "    top5_probs = F.softmax(out_classifier.topk(5).values[0]) * 100.0  # convert to percentage\n",
        "    for idx, prob in zip(top5_indices, top5_probs):\n",
        "        str_idx = str(idx.item())\n",
        "        print(f\"{SOMETHING_SOMETHING_V2_CLASSES[str_idx]} ({prob}%)\")\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcMXyzEEl4_t"
      },
      "source": [
        "Next, let's download a sample video to the local repository. If the video is already downloaded, the code will skip this step. Likewise, let's download a mapping for the action recognition classes used in Something-Something V2, so we can interpret the predicted action class from our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "GGgLV-Fil4_t",
        "outputId": "3534d589-81a2-4c0b-d0da-890f042fc0c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading video\n",
            "Downloading SSV2 classes\n"
          ]
        }
      ],
      "source": [
        "sample_video_path = \"sample_video.mp4\"\n",
        "# Download the video if not yet downloaded to local path\n",
        "if not os.path.exists(sample_video_path):\n",
        "    video_url = \"https://huggingface.co/datasets/nateraw/kinetics-mini/resolve/main/val/bowling/-WH-lxmGJVY_000005_000015.mp4\"\n",
        "    command = [\"wget\", video_url, \"-O\", sample_video_path]\n",
        "    subprocess.run(command)\n",
        "    print(\"Downloading video\")\n",
        "\n",
        "# Download SSV2 classes if not already present\n",
        "ssv2_classes_path = \"ssv2_classes.json\"\n",
        "if not os.path.exists(ssv2_classes_path):\n",
        "    command = [\n",
        "        \"wget\",\n",
        "        \"https://huggingface.co/datasets/huggingface/label-files/resolve/d79675f2d50a7b1ecf98923d42c30526a51818e2/\"\n",
        "        \"something-something-v2-id2label.json\",\n",
        "        \"-O\",\n",
        "        \"ssv2_classes.json\",\n",
        "    ]\n",
        "    subprocess.run(command)\n",
        "    print(\"Downloading SSV2 classes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQG6wnxCl4_t"
      },
      "source": [
        "Now, let's load the models in both vanilla Pytorch as well as through the HuggingFace API. Note that HuggingFace API will automatically load the weights through `from_pretrained()`, so there is no additional download required for HuggingFace.\n",
        "\n",
        "To download the PyTorch model weights, use wget and specify your preferred target path. See the README for the model weight URLs.\n",
        "E.g.\n",
        "```\n",
        "wget https://dl.fbaipublicfiles.com/vjepa2/vitg-384.pt -P YOUR_DIR\n",
        "```\n",
        "Then update `pt_model_path` with `YOUR_DIR/vitg-384.pt`. Also note that you have the option to use `torch.hub.load`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "cy7x8Tckl4_t"
      },
      "outputs": [],
      "source": [
        "# HuggingFace model repo name\n",
        "hf_model_name = (\n",
        "    \"facebook/vjepa2-vitg-fpc64-384\"  # Replace with your favored model, e.g. facebook/vjepa2-vitg-fpc64-384\n",
        ")\n",
        "# Path to local PyTorch weights\n",
        "#pt_model_path = \"YOUR_MODEL_PATH\"\n",
        "\n",
        "# Initialize the HuggingFace model, load pretrained weights\n",
        "model_hf = AutoModel.from_pretrained(hf_model_name)\n",
        "model_hf.cuda().eval()\n",
        "\n",
        "# Build HuggingFace preprocessing transform\n",
        "hf_transform = AutoVideoProcessor.from_pretrained(hf_model_name)\n",
        "img_size = hf_transform.crop_size[\"height\"]  # E.g. 384, 256, etc.\n",
        "\n",
        "# Initialize the PyTorch model, load pretrained weights\n",
        "#model_pt = vit_giant_xformers_rope(img_size=(img_size, img_size), num_frames=64)\n",
        "#model_pt.cuda().eval()\n",
        "#load_pretrained_vjepa_pt_weights(model_pt, pt_model_path)\n",
        "\n",
        "### Can also use torch.hub to load the model\n",
        "# model_pt, _ = torch.hub.load('facebookresearch/vjepa2', 'vjepa2_vit_giant_384')\n",
        "# model_pt.cuda().eval()\n",
        "\n",
        "# Build PyTorch preprocessing transform\n",
        "pt_video_transform = build_pt_video_transform(img_size=img_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XG9BzpgGl4_u"
      },
      "source": [
        "Now we can run the encoder on the video to get the patch-wise features from the last layer of the encoder. To verify that the HuggingFace and PyTorch models are equivalent, we will compare the values of the features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "9Hr2CTKCl4_u",
        "outputId": "d8613164-22e2-4b46-c4a8-594193c818ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
            "  self.gen = func(*args, **kwds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Inference results on video:\n",
            "    HuggingFace output shape: torch.Size([1, 18432, 1408])\n",
            "    PyTorch output shape:     torch.Size([1, 18432, 1408])\n",
            "    Absolute difference sum:  54907976.000000\n",
            "    Close: False\n",
            "    \n"
          ]
        }
      ],
      "source": [
        "# Inference on video to get the patch-wise features\n",
        "out_patch_features_hf, out_patch_features_pt = forward_vjepa_video(\n",
        "    model_hf, model_pt, hf_transform, pt_video_transform\n",
        ")\n",
        "\n",
        "print(\n",
        "    f\"\"\"\n",
        "    Inference results on video:\n",
        "    HuggingFace output shape: {out_patch_features_hf.shape}\n",
        "    PyTorch output shape:     {out_patch_features_pt.shape}\n",
        "    Absolute difference sum:  {torch.abs(out_patch_features_pt - out_patch_features_hf).sum():.6f}\n",
        "    Close: {torch.allclose(out_patch_features_pt, out_patch_features_hf, atol=1e-3, rtol=1e-3)}\n",
        "    \"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the attentive probe classifier weights for Something-Something V2\n",
        "!wget https://dl.fbaipublicfiles.com/vjepa2/evals/ssv2-vitg-384-64x2x3.pt -P /content/classifiers\n",
        "\n",
        "# Set the classifier path\n",
        "classifier_model_path = \"/content/classifiers/ssv2-vitg-384-64x2x3.pt\"\n",
        "\n",
        "# Initialize the classifier\n",
        "# Note: Since you're using HuggingFace model, use model_hf.config.hidden_size instead of model_pt.embed_dim\n",
        "classifier = (\n",
        "    AttentiveClassifier(\n",
        "        embed_dim=model_hf.config.hidden_size,  # or use 1408 directly\n",
        "        num_heads=16,\n",
        "        depth=4,\n",
        "        num_classes=174\n",
        "    ).cuda().eval()\n",
        ")\n",
        "load_pretrained_vjepa_classifier_weights(classifier, classifier_model_path)\n",
        "\n",
        "# Get classification results (use HuggingFace features)\n",
        "get_vjepa_video_classification_results(classifier, out_patch_features_hf)"
      ],
      "metadata": {
        "id": "9nrHC5TSmwY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-tuJlzxl4_u"
      },
      "source": [
        "Great! Now we know that the features from both models are equivalent. Now let's run a pretrained attentive probe classifier on top of the extracted features, to predict an action class for the video. Let's use the Something-Something V2 probe. Note that the repository also includes attentive probe weights for other evaluations such as EPIC-KITCHENS-100 and Diving48.\n",
        "\n",
        "To download the attentive probe weights, use wget and specify your preferred target path. E.g. `wget https://dl.fbaipublicfiles.com/vjepa2/evals/ssv2-vitg-384-64x2x3.pt -P YOUR_DIR`\n",
        "\n",
        "Then update `classifier_model_path` with `YOUR_DIR/ssv2-vitg-384-64x2x3.pt`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "LBF91S-Ql4_u",
        "outputId": "70531708-a7e2-4848-fdc1-118427558d29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-01-29 02:36:28--  https://dl.fbaipublicfiles.com/vjepa2/evals/ssv2-vitg-384-64x2x3.pt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.171.22.13, 3.171.22.33, 3.171.22.118, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.171.22.13|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 373996872 (357M) [application/vnd.snesdev-page-table]\n",
            "Saving to: /content/classifiers/ssv2-vitg-384-64x2x3.pt.2\n",
            "\n",
            "ssv2-vitg-384-64x2x 100%[===================>] 356.67M   294MB/s    in 1.2s    \n",
            "\n",
            "2026-01-29 02:36:29 (294 MB/s) - /content/classifiers/ssv2-vitg-384-64x2x3.pt.2 saved [373996872/373996872]\n",
            "\n",
            "Pretrained weights found at /content/classifiers/ssv2-vitg-384-64x2x3.pt and loaded with msg: <All keys matched successfully>\n",
            "Classifier output shape: torch.Size([1, 174])\n",
            "Top 5 predicted class names:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2987202713.py:85: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  top5_probs = F.softmax(out_classifier.topk(5).values[0]) * 100.0  # convert to percentage\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moving [something] away from the camera (25.095457077026367%)\n",
            "Burying [something] in [something] (19.680456161499023%)\n",
            "Pretending to pick [something] up (19.282258987426758%)\n",
            "Putting [something] on a surface (18.631010055541992%)\n",
            "Pretending to turn [something] upside down (17.310815811157227%)\n"
          ]
        }
      ],
      "source": [
        "# Initialize the classifier\n",
        "#classifier_model_path = \"YOUR_ATTENTIVE_PROBE_PATH\"\n",
        "\n",
        "# Download the attentive probe classifier weights for Something-Something V2\n",
        "!wget https://dl.fbaipublicfiles.com/vjepa2/evals/ssv2-vitg-384-64x2x3.pt -P /content/classifiers\n",
        "\n",
        "# Set the classifier path\n",
        "classifier_model_path = \"/content/classifiers/ssv2-vitg-384-64x2x3.pt\"\n",
        "\n",
        "classifier = (\n",
        "    AttentiveClassifier(embed_dim=model_pt.embed_dim, num_heads=16, depth=4, num_classes=174).cuda().eval()\n",
        ")\n",
        "load_pretrained_vjepa_classifier_weights(classifier, classifier_model_path)\n",
        "\n",
        "# Get classification results\n",
        "get_vjepa_video_classification_results(classifier, out_patch_features_pt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezu3s6yll4_u"
      },
      "source": [
        "The video features a man putting a bowling ball into a tube, so the predicted action of \"Putting [something] into [something]\" makes sense!\n",
        "\n",
        "This concludes the tutorial. Please see the README and paper for full details on the capabilities of V-JEPA 2 :)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing on our own video"
      ],
      "metadata": {
        "id": "cawsYHxInE2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Direct path to your video file\n",
        "video_path = \"/content/episode_000000.mp4\"\n",
        "\n",
        "print(f\"\\nProcessing video: {video_path}\")\n",
        "\n",
        "# Load video\n",
        "video_reader = VideoReader(video_path)\n",
        "frames = [frame.asnumpy() for frame in video_reader]\n",
        "\n",
        "# Preprocess with HuggingFace\n",
        "inputs = hf_transform(frames, return_tensors=\"pt\")\n",
        "inputs = {k: v.cuda() for k, v in inputs.items()}\n",
        "\n",
        "# Get features\n",
        "with torch.no_grad():\n",
        "    outputs = model_hf(**inputs)\n",
        "    out_features_custom = outputs.last_hidden_state\n",
        "\n",
        "print(f\"Extracted features shape: {out_features_custom.shape}\")\n",
        "\n",
        "# Run classification\n",
        "get_vjepa_video_classification_results(classifier, out_features_custom)"
      ],
      "metadata": {
        "id": "1HnUbkt9nNjG",
        "outputId": "98320cdf-cdc9-45df-a356-8d86e77d18b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing video: /content/episode_000000.mp4\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "DECORDError",
          "evalue": "[02:38:41] /github/workspace/src/video/video_reader.cc:151: Check failed: st_nb >= 0 (-1128613112 vs. 0) ERROR cannot find video stream with wanted index: -1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mDECORDError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3357625877.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load video\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mvideo_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVideoReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvideo_reader\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/decord/video_reader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, uri, ctx, width, height, num_threads, fault_tol)\u001b[0m\n\u001b[1;32m     52\u001b[0m                 ba, ctx.device_type, ctx.device_id, width, height, num_threads, 2, fault_tol)\n\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             self._handle = _CAPI_VideoReaderGetVideoReader(\n\u001b[0m\u001b[1;32m     55\u001b[0m                 uri, ctx.device_type, ctx.device_id, width, height, num_threads, 0, fault_tol)\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/decord/_ffi/_ctypes/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mret_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDECORDValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mret_tcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         check_call(_LIB.DECORDFuncCall(\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtcodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             ctypes.byref(ret_val), ctypes.byref(ret_tcode)))\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/decord/_ffi/base.py\u001b[0m in \u001b[0;36mcheck_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0merr_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merr_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Stack trace'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mDECORDLimitReachedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mDECORDError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDECORDError\u001b[0m: [02:38:41] /github/workspace/src/video/video_reader.cc:151: Check failed: st_nb >= 0 (-1128613112 vs. 0) ERROR cannot find video stream with wanted index: -1"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Debugging why it can't read video"
      ],
      "metadata": {
        "id": "3uWDSXAGnwXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test and process video with better error handling\n",
        "video_path = \"/content/episode_000000.mp4\"\n",
        "\n",
        "print(f\"Processing video: {video_path}\")\n",
        "\n",
        "# First, let's check if the file exists and get info\n",
        "import os\n",
        "print(f\"File exists: {os.path.exists(video_path)}\")\n",
        "print(f\"File size: {os.path.getsize(video_path) / (1024*1024):.2f} MB\")\n",
        "\n",
        "# Try to get video info using ffmpeg\n",
        "!ffprobe -v error -show_entries stream=codec_name,width,height,r_frame_rate -of default=noprint_wrappers=1 \"{video_path}\"\n",
        "\n",
        "# Try decord with specific context\n",
        "try:\n",
        "    from decord import VideoReader, cpu\n",
        "    video_reader = VideoReader(video_path, ctx=cpu(0))\n",
        "    print(f\"Video loaded successfully!\")\n",
        "    print(f\"Number of frames: {len(video_reader)}\")\n",
        "\n",
        "    # Sample frames instead of loading all\n",
        "    num_frames = min(64, len(video_reader))  # V-JEPA2 uses 64 frames\n",
        "    indices = list(range(0, len(video_reader), len(video_reader) // num_frames))[:num_frames]\n",
        "    frames = video_reader.get_batch(indices).asnumpy()\n",
        "\n",
        "    print(f\"Extracted {len(frames)} frames\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Decord failed: {e}\")\n",
        "    print(\"\\nTrying alternative method with OpenCV...\")\n",
        "\n",
        "    # Fallback to OpenCV\n",
        "    import cv2\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "\n",
        "    while len(frames) < 64 and cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        # Convert BGR to RGB\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frames.append(frame)\n",
        "\n",
        "    cap.release()\n",
        "    print(f\"Extracted {len(frames)} frames with OpenCV\")\n",
        "\n",
        "# Continue with processing\n",
        "if len(frames) > 0:\n",
        "    # Preprocess with HuggingFace\n",
        "    inputs = hf_transform(frames, return_tensors=\"pt\")\n",
        "    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
        "\n",
        "    # Get features\n",
        "    with torch.no_grad():\n",
        "        outputs = model_hf(**inputs)\n",
        "        out_features_custom = outputs.last_hidden_state\n",
        "\n",
        "    print(f\"Extracted features shape: {out_features_custom.shape}\")\n",
        "\n",
        "    # Run classification\n",
        "    get_vjepa_video_classification_results(classifier, out_features_custom)\n",
        "else:\n",
        "    print(\"Failed to extract frames from video\")"
      ],
      "metadata": {
        "id": "jWmoUq8Sn0kw",
        "outputId": "a1e3239a-f3e6-4333-f4a0-690e14eaf311",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing video: /content/episode_000000.mp4\n",
            "File exists: True\n",
            "File size: 1.75 MB\n",
            "codec_name=av1\n",
            "width=640\n",
            "height=480\n",
            "r_frame_rate=30/1\n",
            "Decord failed: [02:40:09] /github/workspace/src/video/video_reader.cc:151: Check failed: st_nb >= 0 (-1128613112 vs. 0) ERROR cannot find video stream with wanted index: -1\n",
            "\n",
            "Trying alternative method with OpenCV...\n",
            "Extracted 0 frames with OpenCV\n",
            "Failed to extract frames from video\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert AV1 video to H.264 format that decord can read\n",
        "video_path = \"/content/episode_000000.mp4\"\n",
        "converted_path = \"/content/episode_000000_converted.mp4\"\n",
        "\n",
        "print(\"Converting video to compatible format...\")\n",
        "!ffmpeg -i \"{video_path}\" -c:v libx264 -pix_fmt yuv420p -c:a aac \"{converted_path}\" -y\n",
        "\n",
        "print(f\"\\nProcessing converted video: {converted_path}\")\n",
        "\n",
        "# Now try loading the converted video\n",
        "from decord import VideoReader, cpu\n",
        "\n",
        "try:\n",
        "    video_reader = VideoReader(converted_path, ctx=cpu(0))\n",
        "    print(f\"Video loaded successfully!\")\n",
        "    print(f\"Number of frames: {len(video_reader)}\")\n",
        "\n",
        "    # Sample frames (V-JEPA2 typically uses 64 frames)\n",
        "    num_frames = min(64, len(video_reader))\n",
        "    indices = list(range(0, len(video_reader), len(video_reader) // num_frames))[:num_frames]\n",
        "    frames = video_reader.get_batch(indices).asnumpy()\n",
        "\n",
        "    print(f\"Extracted {len(frames)} frames, shape: {frames.shape}\")\n",
        "\n",
        "    # Preprocess with HuggingFace\n",
        "    inputs = hf_transform(frames, return_tensors=\"pt\")\n",
        "    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
        "\n",
        "    # Get features\n",
        "    with torch.no_grad():\n",
        "        outputs = model_hf(**inputs)\n",
        "        out_features_custom = outputs.last_hidden_state\n",
        "\n",
        "    print(f\"Extracted features shape: {out_features_custom.shape}\")\n",
        "\n",
        "    # Run classification\n",
        "    get_vjepa_video_classification_results(classifier, out_features_custom)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "id": "LG9w5RhFn-_x",
        "outputId": "e59c2656-b781-4f7e-f889-3fc47b9086e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting video to compatible format...\n",
            "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 70.100 / 56. 70.100\n",
            "  libavcodec     58.134.100 / 58.134.100\n",
            "  libavformat    58. 76.100 / 58. 76.100\n",
            "  libavdevice    58. 13.100 / 58. 13.100\n",
            "  libavfilter     7.110.100 /  7.110.100\n",
            "  libswscale      5.  9.100 /  5.  9.100\n",
            "  libswresample   3.  9.100 /  3.  9.100\n",
            "  libpostproc    55.  9.100 / 55.  9.100\n",
            "\u001b[1;36m[libdav1d @ 0x55916279dc40] \u001b[0mlibdav1d 0.9.2\n",
            "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/content/episode_000000.mp4':\n",
            "  Metadata:\n",
            "    major_brand     : isom\n",
            "    minor_version   : 512\n",
            "    compatible_brands: isomav01iso2mp41\n",
            "    encoder         : Lavf62.3.100\n",
            "  Duration: 00:00:03.67, start: 0.000000, bitrate: 4001 kb/s\n",
            "  Stream #0:0(und): Video: av1 (Main) (av01 / 0x31307661), yuv420p(tv), 640x480 [SAR 1:1 DAR 4:3], 3998 kb/s, 30 fps, 30 tbr, 15360 tbn, 15360 tbc (default)\n",
            "    Metadata:\n",
            "      handler_name    : VideoHandler\n",
            "      vendor_id       : [0][0][0][0]\n",
            "\u001b[1;36m[libdav1d @ 0x5591627be500] \u001b[0mlibdav1d 0.9.2\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (av1 (libdav1d) -> h264 (libx264))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;36m[libx264 @ 0x5591627a2140] \u001b[0musing SAR=1/1\n",
            "\u001b[1;36m[libx264 @ 0x5591627a2140] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2 AVX512\n",
            "\u001b[1;36m[libx264 @ 0x5591627a2140] \u001b[0mprofile High, level 3.0, 4:2:0, 8-bit\n",
            "\u001b[1;36m[libx264 @ 0x5591627a2140] \u001b[0m264 - core 163 r3060 5db6aa6 - H.264/MPEG-4 AVC codec - Copyleft 2003-2021 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=15 lookahead_threads=2 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
            "Output #0, mp4, to '/content/episode_000000_converted.mp4':\n",
            "  Metadata:\n",
            "    major_brand     : isom\n",
            "    minor_version   : 512\n",
            "    compatible_brands: isomav01iso2mp41\n",
            "    encoder         : Lavf58.76.100\n",
            "  Stream #0:0(und): Video: h264 (avc1 / 0x31637661), yuv420p(tv, progressive), 640x480 [SAR 1:1 DAR 4:3], q=2-31, 30 fps, 15360 tbn (default)\n",
            "    Metadata:\n",
            "      handler_name    : VideoHandler\n",
            "      vendor_id       : [0][0][0][0]\n",
            "      encoder         : Lavc58.134.100 libx264\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\n",
            "frame=  110 fps=0.0 q=-1.0 Lsize=     404kB time=00:00:03.56 bitrate= 928.6kbits/s speed=5.22x    \n",
            "video:402kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.523490%\n",
            "\u001b[1;36m[libx264 @ 0x5591627a2140] \u001b[0mframe I:1     Avg QP:23.92  size: 20391\n",
            "\u001b[1;36m[libx264 @ 0x5591627a2140] \u001b[0mframe P:34    Avg QP:24.71  size:  7751\n",
            "\u001b[1;36m[libx264 @ 0x5591627a2140] \u001b[0mframe B:75    Avg QP:27.47  size:  1696\n",
            "\u001b[1;36m[libx264 @ 0x5591627a2140] \u001b[0mconsecutive B-frames:  5.5%  7.3% 10.9% 76.4%\n",
            "\u001b[1;36m[libx264 @ 0x5591627a2140] \u001b[0mmb I  I16..4: 11.5% 72.2% 16.3%\n",
            "\u001b[1;36m[libx264 @ 0x5591627a2140] \u001b[0mmb P  I16..4:  2.3%  9.2%  1.8%  P16..4: 53.2% 12.3%  8.3%  0.0%  0.0%    skip:12.9%\n",
            "\u001b[1;36m[libx264 @ 0x5591627a2140] \u001b[0mmb B  I16..4:  0.2%  0.8%  0.2%  B16..8: 31.6%  3.5%  0.6%  direct: 2.9%  skip:60.1%  L0:43.3% L1:50.0% BI: 6.7%\n",
            "\u001b[1;36m[libx264 @ 0x5591627a2140] \u001b[0m8x8 transform intra:69.2% inter:82.8%\n",
            "\u001b[1;36m[libx264 @ 0x5591627a2140] \u001b[0mcoded y,uvDC,uvAC intra: 62.7% 93.4% 65.5% inter: 12.6% 33.7% 7.9%\n",
            "\u001b[1;36m[libx264 @ 0x5591627a2140] \u001b[0mi16 v,h,dc,p: 22% 21%  7% 49%\n",
            "\u001b[1;36m[libx264 @ 0x5591627a2140] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 18% 17% 15%  6%  9% 13%  6%  9%  7%\n",
            "\u001b[1;36m[libx264 @ 0x5591627a2140] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 22% 13% 15%  7% 11% 13%  5%  9%  4%\n",
            "\u001b[1;36m[libx264 @ 0x5591627a2140] \u001b[0mi8c dc,h,v,p: 52% 17% 21% 10%\n",
            "\u001b[1;36m[libx264 @ 0x5591627a2140] \u001b[0mWeighted P-Frames: Y:20.6% UV:17.6%\n",
            "\u001b[1;36m[libx264 @ 0x5591627a2140] \u001b[0mref P L0: 57.3% 16.5% 18.1%  7.0%  1.2%\n",
            "\u001b[1;36m[libx264 @ 0x5591627a2140] \u001b[0mref B L0: 87.7%  8.7%  3.6%\n",
            "\u001b[1;36m[libx264 @ 0x5591627a2140] \u001b[0mref B L1: 97.5%  2.5%\n",
            "\u001b[1;36m[libx264 @ 0x5591627a2140] \u001b[0mkb/s:897.08\n",
            "\n",
            "Processing converted video: /content/episode_000000_converted.mp4\n",
            "Video loaded successfully!\n",
            "Number of frames: 110\n",
            "Extracted 64 frames, shape: (64, 480, 640, 3)\n",
            "Extracted features shape: torch.Size([1, 18432, 1408])\n",
            "Classifier output shape: torch.Size([1, 174])\n",
            "Top 5 predicted class names:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2987202713.py:85: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  top5_probs = F.softmax(out_classifier.topk(5).values[0]) * 100.0  # convert to percentage\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Holding [something] (35.198081970214844%)\n",
            "Covering [something] with [something] (18.10696029663086%)\n",
            "Pouring [something] into [something] (17.14968490600586%)\n",
            "Moving [something] down (16.28497886657715%)\n",
            "Holding [something] over [something] (13.260290145874023%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizing labels availble in something to something dataset.\n",
        "Link: https://www.qualcomm.com/developer/software/something-something-v-2-dataset"
      ],
      "metadata": {
        "id": "JIYRuG8osJNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The SSv2 labels are embedded in the dataset\n",
        "\n",
        "ssv2_labels = {\n",
        "    0: \"Approaching [something] with your camera\",\n",
        "    1: \"Attaching [something] to [something]\",\n",
        "    2: \"Bending [something] so that it deforms\",\n",
        "    3: \"Bending [something] until it breaks\",\n",
        "    4: \"Burying [something] in [something]\",\n",
        "    5: \"Closing [something]\",\n",
        "    6: \"Covering [something] with [something]\",\n",
        "    7: \"Digging [something] out of [something]\",\n",
        "    8: \"Dropping [something] behind [something]\",\n",
        "    9: \"Dropping [something] in front of [something]\",\n",
        "    10: \"Dropping [something] into [something]\",\n",
        "    11: \"Dropping [something] next to [something]\",\n",
        "    12: \"Dropping [something] onto [something]\",\n",
        "    13: \"Failing to put [something] into [something] because [something] does not fit\",\n",
        "    14: \"Folding [something]\",\n",
        "    15: \"Hitting [something] with [something]\",\n",
        "    16: \"Holding [something]\",\n",
        "    17: \"Holding [something] behind [something]\",\n",
        "    18: \"Holding [something] in front of [something]\",\n",
        "    19: \"Holding [something] next to [something]\",\n",
        "    20: \"Holding [something] over [something]\",\n",
        "    21: \"Laying [something] on the table on its side, not upright\",\n",
        "    22: \"Letting [something] roll along a flat surface\",\n",
        "    23: \"Letting [something] roll down a slanted surface\",\n",
        "    24: \"Letting [something] roll up a slanted surface, so it rolls back down\",\n",
        "    25: \"Lifting a surface with [something] on it but not enough for it to slide down\",\n",
        "    26: \"Lifting a surface with [something] on it until it starts sliding down\",\n",
        "    27: \"Lifting up one end of [something without letting it drop down]\",\n",
        "    28: \"Lifting up one end of [something], then letting it drop down\",\n",
        "    29: \"Moving [something] across a surface until it falls down\",\n",
        "    30: \"Moving [something] across a surface without it falling down\",\n",
        "    31: \"Moving [something] and [something] away from each other\",\n",
        "    32: \"Moving [something] and [something] closer to each other\",\n",
        "    33: \"Moving [something] and [something] so they collide with each other\",\n",
        "    34: \"Moving [something] and [something] so they pass each other\",\n",
        "    35: \"Moving [something] away from [something]\",\n",
        "    36: \"Moving [something] away from the camera\",\n",
        "    37: \"Moving [something] closer to [something]\",\n",
        "    38: \"Moving [something] down\",\n",
        "    39: \"Moving [something] towards the camera\",\n",
        "    40: \"Moving [something] up\",\n",
        "    41: \"Opening [something]\",\n",
        "    42: \"Picking [something] up\",\n",
        "    43: \"Piling [something] up\",\n",
        "    44: \"Plugging [something] into [something]\",\n",
        "    45: \"Plugging [something] into [something] but pulling it right out as you remove your hand\",\n",
        "    46: \"Poking [something] so lightly that it doesn't or almost doesn't move\",\n",
        "    47: \"Poking [something] so that it falls over\",\n",
        "    48: \"Poking [something] so that it slightly moves\",\n",
        "    49: \"Poking [something] so that it spins around\",\n",
        "    50: \"Poking a hole into [something soft]\",\n",
        "    51: \"Poking a hole into some substance\",\n",
        "    52: \"Poking a stack of [something] so the stack collapses\",\n",
        "    53: \"Poking a stack of [something] without the stack collapsing\",\n",
        "    54: \"Pouring [something] into [something]\",\n",
        "    55: \"Pouring [something] into [something] until it overflows\",\n",
        "    56: \"Pouring [something] onto [something]\",\n",
        "    57: \"Pouring [something] out of [something]\",\n",
        "    58: \"Pretending or failing to wipe [something] off of [something]\",\n",
        "    59: \"Pretending or trying and failing to twist [something]\",\n",
        "    60: \"Pretending to be tearing [something that is not tearable]\",\n",
        "    61: \"Pretending to close [something] without actually closing it\",\n",
        "    62: \"Pretending to open [something] without actually opening it\",\n",
        "    63: \"Pretending to pick [something] up\",\n",
        "    64: \"Pretending to poke [something]\",\n",
        "    65: \"Pretending to pour [something] out of [something], but [something] is empty\",\n",
        "    66: \"Pretending to put [something] behind [something]\",\n",
        "    67: \"Pretending to put [something] into [something]\",\n",
        "    68: \"Pretending to put [something] next to [something]\",\n",
        "    69: \"Pretending to put [something] on a surface\",\n",
        "    70: \"Pretending to put [something] onto [something]\",\n",
        "    71: \"Pretending to scoop [something] up with [something]\",\n",
        "    72: \"Pretending to spread air onto [something]\",\n",
        "    73: \"Pretending to sprinkle air onto [something]\",\n",
        "    74: \"Pretending to squeeze [something]\",\n",
        "    75: \"Pretending to take [something] from [somewhere]\",\n",
        "    76: \"Pretending to take [something] out of [something]\",\n",
        "    77: \"Pretending to throw [something]\",\n",
        "    78: \"Pretending to turn [something] upside down\",\n",
        "    79: \"Pulling [something] from behind of [something]\",\n",
        "    80: \"Pulling [something] from left to right\",\n",
        "    81: \"Pulling [something] from right to left\",\n",
        "    82: \"Pulling [something] onto [something]\",\n",
        "    83: \"Pulling [something] out of [something]\",\n",
        "    84: \"Pulling two ends of [something] so that it gets stretched\",\n",
        "    85: \"Pulling two ends of [something] so that it separates into two pieces\",\n",
        "    86: \"Pushing [something] from left to right\",\n",
        "    87: \"Pushing [something] from right to left\",\n",
        "    88: \"Pushing [something] off of [something]\",\n",
        "    89: \"Pushing [something] onto [something]\",\n",
        "    90: \"Pushing [something] so it spins\",\n",
        "    91: \"Pushing [something] so that it almost falls off but doesn't\",\n",
        "    92: \"Pushing [something] so that it falls off the table\",\n",
        "    93: \"Pushing [something] so that it slightly moves\",\n",
        "    94: \"Pushing [something] with [something]\",\n",
        "    95: \"Putting [number of] [something] onto [something]\",\n",
        "    96: \"Putting [something] and [something] on the table\",\n",
        "    97: \"Putting [something] behind [something]\",\n",
        "    98: \"Putting [something] in front of [something]\",\n",
        "    99: \"Putting [something] into [something]\",\n",
        "    100: \"Putting [something] next to [something]\",\n",
        "    101: \"Putting [something] on a flat surface without letting it roll\",\n",
        "    102: \"Putting [something] on a surface\",\n",
        "    103: \"Putting [something] on the edge of [something] so it is not supported and falls down\",\n",
        "    104: \"Putting [something] onto [something]\",\n",
        "    105: \"Putting [something] onto [something] else that cannot support it so it falls down\",\n",
        "    106: \"Putting [something] onto a slanted surface but it doesn't glide down\",\n",
        "    107: \"Putting [something] similar to other things that are already on the table\",\n",
        "    108: \"Putting [something] that can't roll onto a slanted surface, so it slides down\",\n",
        "    109: \"Putting [something] that can't roll onto a slanted surface, so it stays where it is\",\n",
        "    110: \"Putting [something] that cannot actually stand upright upright on the table, so it falls on its side\",\n",
        "    111: \"Putting [something], [something] and [something] on the table\",\n",
        "    112: \"Putting [something] upright on the table, so it falls on its side\",\n",
        "    113: \"Putting [something] underneath [something]\",\n",
        "    114: \"Removing [something], revealing [something] behind\",\n",
        "    115: \"Rolling [something] on a flat surface\",\n",
        "    116: \"Scooping [something] up with [something]\",\n",
        "    117: \"Showing [something] behind [something]\",\n",
        "    118: \"Showing [something] next to [something]\",\n",
        "    119: \"Showing [something] on top of [something]\",\n",
        "    120: \"Showing [something] to the camera\",\n",
        "    121: \"Showing that [something] is empty\",\n",
        "    122: \"Showing that [something] is inside [something]\",\n",
        "    123: \"Something being deflected from [something]\",\n",
        "    124: \"Spilling [something] behind [something]\",\n",
        "    125: \"Spilling [something] next to [something]\",\n",
        "    126: \"Spilling [something] onto [something]\",\n",
        "    127: \"Spinning [something] so it continues spinning\",\n",
        "    128: \"Spinning [something] that quickly stops spinning\",\n",
        "    129: \"Spreading [something] onto [something]\",\n",
        "    130: \"Sprinkling [something] onto [something]\",\n",
        "    131: \"Squeezing [something]\",\n",
        "    132: \"Stacking [number of] [something]\",\n",
        "    133: \"Stuffing [something] into [something]\",\n",
        "    134: \"Taking [something] from [somewhere]\",\n",
        "    135: \"Taking [something] out of [something]\",\n",
        "    136: \"Tearing [something] into two pieces\",\n",
        "    137: \"Tearing [something] just a little bit\",\n",
        "    138: \"Throwing [something]\",\n",
        "    139: \"Throwing [something] against [something]\",\n",
        "    140: \"Throwing [something] in the air and catching it\",\n",
        "    141: \"Throwing [something] in the air and letting it fall\",\n",
        "    142: \"Throwing [something] onto a surface\",\n",
        "    143: \"Tilting [something] with [something] on it slightly so it doesn't fall down\",\n",
        "    144: \"Tilting [something] with [something] on it until it falls off\",\n",
        "    145: \"Tipping [something] over\",\n",
        "    146: \"Tipping [something] with [something] in it over, so [something] falls out\",\n",
        "    147: \"Touching (without moving) part of [something]\",\n",
        "    148: \"Trying but failing to attach [something] to [something] because it doesn't stick\",\n",
        "    149: \"Trying to bend [something unbendable] so nothing happens\",\n",
        "    150: \"Trying to pour [something] into [something], but missing so it spills next to it\",\n",
        "    151: \"Turning [something] upside down\",\n",
        "    152: \"Turning the camera downwards while filming [something]\",\n",
        "    153: \"Turning the camera left while filming [something]\",\n",
        "    154: \"Turning the camera right while filming [something]\",\n",
        "    155: \"Turning the camera upwards while filming [something]\",\n",
        "    156: \"Twisting (wringing) [something] wet until water comes out\",\n",
        "    157: \"Twisting [something]\",\n",
        "    158: \"Uncovering [something]\",\n",
        "    159: \"Unfolding [something]\",\n",
        "    160: \"Wiping [something] off of [something]\",\n",
        "    161: \"Wiping [something] with [something]\",\n",
        "    162: \"Holding [something] over [something]\",\n",
        "    163: \"Pouring [something] into [something]\",\n",
        "    164: \"Pouring [something] out of [something]\",\n",
        "    165: \"Pretending to put [something] onto [something]\",\n",
        "    166: \"Putting [something] into [something]\",\n",
        "    167: \"Putting [something] onto [something]\",\n",
        "    168: \"Showing [something] to the camera\",\n",
        "    169: \"Tilting [something] with [something] on it until it falls off\",\n",
        "    170: \"Approaching [something] with your camera\",\n",
        "    171: \"Moving [something] closer to [something]\",\n",
        "    172: \"Putting [something] onto [something]\",\n",
        "    173: \"Showing [something] to the camera\"\n",
        "}\n",
        "\n",
        "# Now map your predictions to actual labels\n",
        "top_indices_from_your_video = [16, 6, 54, 38, 20]  # From your output\n",
        "\n",
        "print(\"Your video predictions with actual class names:\")\n",
        "for i, idx in enumerate(top_indices_from_your_video, 1):\n",
        "    print(f\"{i}. Class {idx}: {ssv2_labels[idx]}\")"
      ],
      "metadata": {
        "id": "S8EfAMzZslQJ",
        "outputId": "b4016244-ab28-4386-9b77-7842cdd74420",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your video predictions with actual class names:\n",
            "1. Class 16: Holding [something]\n",
            "2. Class 6: Covering [something] with [something]\n",
            "3. Class 54: Pouring [something] into [something]\n",
            "4. Class 38: Moving [something] down\n",
            "5. Class 20: Holding [something] over [something]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "fileHeader": "",
    "fileUid": "f0b70ba6-1c84-47e1-81bd-b7642f9acf50",
    "isAdHoc": false,
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}